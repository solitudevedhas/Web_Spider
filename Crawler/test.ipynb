{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from queue import Queue\n",
    "from general import *\n",
    "from domain import *\n",
    "from urllib.request import urlopen\n",
    "from link_finder import LinkFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from link_finder import LinkFinder\n",
    "from domain import *\n",
    "from general import *\n",
    "\n",
    "\n",
    "class Spider:\n",
    "    \n",
    "    # Class variables  (shared among all instalces)\n",
    "    project_name = ''\n",
    "    base_url = ''\n",
    "    domain_name = ''\n",
    "    queue_file = ''\n",
    "    crawled_file = ''\n",
    "    queue = set()\n",
    "    crawled = set()\n",
    "\n",
    "    def __init__(self, project_name, base_url, domain_name):\n",
    "        Spider.project_name = project_name\n",
    "        Spider.base_url = base_url\n",
    "        Spider.domain_name = domain_name\n",
    "        Spider.queue_file = Spider.project_name + '/queue.txt'\n",
    "        Spider.crawled_file = Spider.project_name + '/crawled.txt'\n",
    "        self.boot()\n",
    "        self.crawl_page('First spider', Spider.base_url)\n",
    "\n",
    "        \n",
    "    # Creates directory and files for project on first run and starts the spider\n",
    "    @staticmethod\n",
    "    def boot():\n",
    "        create_project_dir(Spider.project_name)\n",
    "        create_data_files(Spider.project_name, Spider.base_url)\n",
    "        Spider.queue = file_to_set(Spider.queue_file)\n",
    "        Spider.crawled = file_to_set(Spider.crawled_file)\n",
    "    \n",
    "    \n",
    "    # Updates user display, fills queue and updates files\n",
    "    @staticmethod\n",
    "    def crawl_page(thread_name, page_url):\n",
    "        if page_url not in Spider.crawled:\n",
    "            print(thread_name + ' now crawling ' + page_url)\n",
    "            print('Queue ' + str(len(Spider.queue)) + ' | Crawled  ' + str(len(Spider.crawled)))\n",
    "            Spider.add_links_to_queue(Spider.gather_links(page_url))\n",
    "            Spider.queue.remove(page_url)\n",
    "            Spider.crawled.add(page_url)\n",
    "            Spider.update_files()\n",
    "    \n",
    "    \n",
    "  # Converts raw response data into readable information and checks for proper html formatting\n",
    "    @staticmethod\n",
    "    def gather_links(page_url):\n",
    "        html_string = ''\n",
    "        try:\n",
    "            response = urlopen(page_url)\n",
    "            if 'text/html' in response.getheader('Content-Type'):\n",
    "                html_bytes = response.read()\n",
    "                html_string = html_bytes.decode(\"utf-8\")\n",
    "            finder = LinkFinder(Spider.base_url, page_url)\n",
    "            finder.feed(html_string)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            return set()\n",
    "        return finder.page_links()\n",
    "\n",
    "    \n",
    "    # Saves queue data to project files\n",
    "    @staticmethod\n",
    "    def add_links_to_queue(links):\n",
    "        for url in links:\n",
    "            if (url in Spider.queue) or (url in Spider.crawled):\n",
    "                continue\n",
    "            if Spider.domain_name != get_domain_name(url):\n",
    "                continue\n",
    "            Spider.queue.add(url)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def update_files():\n",
    "        set_to_file(Spider.queue, Spider.queue_file)\n",
    "        set_to_file(Spider.crawled, Spider.crawled_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'nintendo'\n",
    "HOMEPAGE = 'https://www.nintendo.com/'\n",
    "DOMAIN_NAME = get_domain_name(HOMEPAGE)\n",
    "QUEUE_FILE = PROJECT_NAME + '/queue.txt'\n",
    "CRAWLED_FILE = PROJECT_NAME + '/crawled.txt'\n",
    "NUMBER_OF_THREADS = 8\n",
    "queue = Queue()\n",
    "\n",
    "Spider(PROJECT_NAME, HOMEPAGE, DOMAIN_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
